{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":817,"status":"ok","timestamp":1763423628450,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"},"user_tz":300},"id":"7MHB9pzBEP1R","outputId":"1b1a914b-126f-4c2c-a333-b18e938992ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import torch\n","import pandas as pd\n","import numpy as np\n","from datasets import load_dataset, Dataset\n","from transformers import (\n","    GPT2TokenizerFast,\n","    GPT2LMHeadModel,\n","    Trainer,\n","    TrainingArguments,\n","    StoppingCriteria,\n","    StoppingCriteriaList\n",")\n","#!pip install evaluate\n","import evaluate\n","from codecarbon import EmissionsTracker\n","from google.colab import drive\n","import torch\n","torch.cuda.empty_cache()\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## Configurations"],"metadata":{"id":"VIFvRMdh6Jeu"}},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1763423630543,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"},"user_tz":300},"id":"Ioi9rXhT-7Mw"},"outputs":[],"source":["class Config:\n","    \"\"\"Centralized configuration\"\"\"\n","    BASE_OUTPUT_DIR = \"/content/drive/MyDrive/Carbonemission/Version2\"\n","    RESULTS_CSV = os.path.join(BASE_OUTPUT_DIR, \"gpt2_squadv2_results.csv\")\n","    LOG_DIR = os.path.join(BASE_OUTPUT_DIR, \"codecarbon_logs\")\n","\n","    # Training sizes to run\n","    DATASET_FRACTIONS = [0.25, 0.50, 0.80] #,\n","\n","    # Training hyperparameters\n","    BATCH_SIZE = 8\n","    GRAD_ACCUM = 2\n","    NUM_EPOCHS = 2\n","    LEARNING_RATE = 5e-5\n","    WEIGHT_DECAY = 0.01\n","\n","    # Generation parameters\n","    MAX_NEW_TOKENS = 15\n","    NUM_BEAMS = 4\n","\n","    # Model\n","    MODEL_NAME = \"gpt2\"\n","\n","    @staticmethod\n","    def get_model_dir(fraction):\n","        return os.path.join(Config.BASE_OUTPUT_DIR, f\"gpt2-squadv2-{int(fraction*100)}\")\n"]},{"cell_type":"markdown","source":["## PROMPT Template"],"metadata":{"id":"c1GF3uP96MsT"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"fo6_095i-_ec","executionInfo":{"status":"ok","timestamp":1763423659702,"user_tz":300,"elapsed":8,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"}}},"outputs":[],"source":["class PromptTemplate:\n","    \"\"\"Improved prompts for extractive QA\"\"\"\n","\n","    @staticmethod\n","    def format_training(question, context, answer):\n","        \"\"\"Format for training\"\"\"\n","        return f\"Answer the question by extracting the exact answer from the context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}\"\n","\n","    @staticmethod\n","    def format_inference(question, context):\n","        \"\"\"Format for inference\"\"\"\n","        return f\"Answer the question by extracting the exact answer from the context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"hzr7UxZP_CXV","executionInfo":{"status":"ok","timestamp":1763423661518,"user_tz":300,"elapsed":1,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"}}},"outputs":[],"source":["class StopOnPunctuationOrNewline(StoppingCriteria):\n","    \"\"\"Stop generation at first period, newline, or question mark\"\"\"\n","    def __init__(self, tokenizer):\n","        self.stop_ids = []\n","        for token in ['.', '\\n', '?', '!']:\n","            ids = tokenizer.encode(token, add_special_tokens=False)\n","            if ids:\n","                self.stop_ids.extend(ids)\n","        self.stop_ids = list(set(self.stop_ids))\n","\n","    def __call__(self, input_ids, scores, **kwargs):\n","        # Check if any stop token in last 2 tokens\n","        for stop_id in self.stop_ids:\n","            if stop_id in input_ids[0][-2:]:\n","                return True\n","        return False"]},{"cell_type":"markdown","source":["## DATA PREPROCESSING"],"metadata":{"id":"61X4AEIf6zqU"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"PehhRGBl_Gsd","executionInfo":{"status":"ok","timestamp":1763423663299,"user_tz":300,"elapsed":7,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"}}},"outputs":[],"source":["class DataPreprocessor:\n","    \"\"\"Handle all data preprocessing\"\"\"\n","\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","\n","    def make_example(self, example):\n","        \"\"\"Convert SQuAD example to training format\"\"\"\n","        question = example[\"question\"].strip()\n","        context = example[\"context\"].strip()\n","\n","        # Handle SQuAD v2 unanswerable questions\n","        answers = example.get(\"answers\", {})\n","        answer_text = \"\"\n","        if answers and len(answers.get(\"text\", [])) > 0:\n","            answer_text = answers[\"text\"][0].strip()\n","\n","        # Use improved prompt template\n","        prompt = PromptTemplate.format_inference(question, context)\n","        input_text = PromptTemplate.format_training(question, context, answer_text)\n","\n","        return {\n","            \"input_text\": input_text,\n","            \"prompt\": prompt,\n","            \"answer\": answer_text\n","        }\n","\n","    def preprocess_split(self, data):\n","        \"\"\"Tokenize and prepare data for training\"\"\"\n","        out = {\"input_ids\": [], \"labels\": [], \"attention_mask\": []}\n","\n","        for ex in data:\n","            e = self.make_example(ex)\n","\n","            # Tokenize prompt and full sequence\n","            enc_prompt = self.tokenizer(e[\"prompt\"], add_special_tokens=False)\n","            enc_full = self.tokenizer(e[\"input_text\"], add_special_tokens=False)\n","\n","            input_ids = enc_full[\"input_ids\"]\n","            prompt_len = len(enc_prompt[\"input_ids\"])\n","\n","            # Mask prompt tokens in labels (only train on answer)\n","            labels = [-100] * prompt_len + input_ids[prompt_len:]\n","\n","            assert len(input_ids) == len(labels)\n","\n","            out[\"input_ids\"].append(input_ids)\n","            out[\"labels\"].append(labels)\n","            out[\"attention_mask\"].append([1] * len(input_ids))\n","\n","        return out\n","\n","    def prepare_datasets(self, squad_data, fraction):\n","        \"\"\"Prepare train and validation datasets\"\"\"\n","        train_size = int(fraction * len(squad_data[\"train\"]))\n","        validation_size = len(squad_data[\"validation\"])  # Use full validation\n","\n","        print(f\"  Preparing {fraction*100}% of training data...\")\n","        print(f\"   Training samples: {train_size}\")\n","        print(f\"   Validation samples: {validation_size}\")\n","\n","        train_subset = squad_data[\"train\"].select(range(train_size))\n","        valid_subset = squad_data[\"validation\"].select(range(validation_size))\n","\n","        train_data = self.preprocess_split(train_subset)\n","        valid_data = self.preprocess_split(valid_subset)\n","\n","        train_ds = Dataset.from_dict(train_data)\n","        valid_ds = Dataset.from_dict(valid_data)\n","\n","        return train_ds, valid_ds, train_subset, valid_subset"]},{"cell_type":"markdown","source":["## DATA COLLATOR"],"metadata":{"id":"4dIwRske66bY"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"frPdJgmz_MCo","executionInfo":{"status":"ok","timestamp":1763423667101,"user_tz":300,"elapsed":5,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"}}},"outputs":[],"source":["def collate_fn(batch, tokenizer):\n","    \"\"\"Custom collation with padding\"\"\"\n","    input_ids = [torch.tensor(ex[\"input_ids\"]) for ex in batch]\n","    labels = [torch.tensor(ex[\"labels\"]) for ex in batch]\n","    masks = [torch.tensor(ex[\"attention_mask\"]) for ex in batch]\n","\n","    input_ids = torch.nn.utils.rnn.pad_sequence(\n","        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n","    )\n","    labels = torch.nn.utils.rnn.pad_sequence(\n","        labels, batch_first=True, padding_value=-100\n","    )\n","    masks = torch.nn.utils.rnn.pad_sequence(\n","        masks, batch_first=True, padding_value=0\n","    )\n","\n","    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": masks}"]},{"cell_type":"markdown","source":["## ANSWER GENERATION & POST-PROCESSING"],"metadata":{"id":"psX7G_jh7CqN"}},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1763423670729,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"},"user_tz":300},"id":"j56ZCoXdL8GM"},"outputs":[],"source":["class AnswerGenerator:\n","    \"\"\"Generate and post-process answers\"\"\"\n","\n","    def __init__(self, model, tokenizer):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.stopping_criteria = StoppingCriteriaList([\n","            StopOnPunctuationOrNewline(tokenizer)\n","        ])\n","\n","    def generate(self, prompt):\n","        \"\"\"Generate answer with improved parameters\"\"\"\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n","\n","        outputs = self.model.generate(\n","            **inputs,\n","            max_new_tokens=Config.MAX_NEW_TOKENS,\n","            num_beams=Config.NUM_BEAMS,\n","            early_stopping=True,\n","            do_sample=False,\n","            no_repeat_ngram_size=3,\n","            temperature=1.0,\n","            stopping_criteria=self.stopping_criteria,\n","            eos_token_id=self.tokenizer.eos_token_id,\n","            pad_token_id=self.tokenizer.pad_token_id,\n","        )\n","\n","        # Decode only the generated part\n","        generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n","        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n","\n","        return generated_text"]},{"cell_type":"markdown","source":["## EVALUATION"],"metadata":{"id":"EmVH5Z6R7LAT"}},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1763423674980,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"},"user_tz":300},"id":"Qx1_QadV_O4o"},"outputs":[],"source":["class Evaluator:\n","    \"\"\"Evaluate model on SQuAD v2\"\"\"\n","\n","    def __init__(self, model, tokenizer, dataset):\n","        self.generator = AnswerGenerator(model, tokenizer)\n","        self.dataset = dataset\n","        self.metric = evaluate.load(\"squad_v2\")\n","\n","    def evaluate(self):\n","        \"\"\"Run evaluation and compute metrics\"\"\"\n","        print(\"\\n Evaluating model on validation set...\")\n","\n","        predictions = []\n","        references = []\n","\n","        for i, item in enumerate(self.dataset):\n","            if i % 100 == 0:\n","                print(f\"   Processing {i}/{len(self.dataset)}...\")\n","\n","            prompt = PromptTemplate.format_inference(item['question'], item['context'])\n","            answer = self.generator.generate(prompt)\n","\n","            predictions.append({\n","                \"id\": item[\"id\"],\n","                \"prediction_text\": answer,\n","                \"no_answer_probability\": 0.0\n","            })\n","\n","            references.append({\n","                \"id\": item[\"id\"],\n","                \"answers\": item[\"answers\"]\n","            })\n","\n","        # Compute metrics\n","        results = self.metric.compute(predictions=predictions, references=references)\n","\n","        print(f\"\\n Evaluation complete!\")\n","        print(f\"   F1 Score: {results['f1']:.4f}\")\n","        print(f\"   Exact Match: {results['exact']:.4f}\")\n","\n","        return results"]},{"cell_type":"markdown","source":["## TRAINING PIPELINE"],"metadata":{"id":"YjJSUyrU7WrL"}},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1763423739425,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"},"user_tz":300},"id":"k7tp3wr2_QSW"},"outputs":[],"source":["class TrainingPipeline:\n","    \"\"\"Main training pipeline\"\"\"\n","\n","    def __init__(self, fraction):\n","        self.fraction = fraction\n","        self.output_dir = Config.get_model_dir(fraction)\n","        os.makedirs(self.output_dir, exist_ok=True)\n","        os.makedirs(Config.LOG_DIR, exist_ok=True)\n","\n","    def setup_model_and_tokenizer(self):\n","        \"\"\"Initialize model and tokenizer\"\"\"\n","        print(f\"\\n Setting up {Config.MODEL_NAME}...\")\n","\n","        tokenizer = GPT2TokenizerFast.from_pretrained(Config.MODEL_NAME)\n","        if tokenizer.pad_token is None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","        model = GPT2LMHeadModel.from_pretrained(Config.MODEL_NAME)\n","\n","        return model, tokenizer\n","\n","    def setup_training_args(self):\n","        \"\"\"Configure training arguments\"\"\"\n","        return TrainingArguments(\n","            output_dir=self.output_dir,\n","            overwrite_output_dir=True,\n","            num_train_epochs=Config.NUM_EPOCHS,\n","            per_device_train_batch_size=Config.BATCH_SIZE,\n","            per_device_eval_batch_size=Config.BATCH_SIZE,\n","            eval_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            fp16=torch.cuda.is_available(),\n","            gradient_accumulation_steps=Config.GRAD_ACCUM,\n","            learning_rate=Config.LEARNING_RATE,\n","            weight_decay=Config.WEIGHT_DECAY,\n","            logging_steps=200,\n","            save_total_limit=2,\n","            push_to_hub=False,\n","            report_to=None,\n","        )\n","\n","    def train(self):\n","        \"\"\"Execute full training pipeline\"\"\"\n","        print(f\"\\n{'='*80}\")\n","        print(f\" TRAINING WITH {self.fraction*100}% OF SQUAD V2 DATA\")\n","        print(f\"{'='*80}\")\n","\n","        # Setup\n","        model, tokenizer = self.setup_model_and_tokenizer()\n","\n","        # Load and prepare data\n","        squad_data = load_dataset(\"squad_v2\")\n","        preprocessor = DataPreprocessor(tokenizer)\n","        train_ds, valid_ds, train_subset, valid_subset = preprocessor.prepare_datasets(\n","            squad_data, self.fraction\n","        )\n","\n","        # Setup trainer\n","        training_args = self.setup_training_args()\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_ds,\n","            eval_dataset=valid_ds,\n","            data_collator=lambda batch: collate_fn(batch, tokenizer),\n","        )\n","\n","        # Start emissions tracking\n","        tracker = EmissionsTracker(\n","            project_name=f\"gpt2_squadv2_{int(self.fraction*100)}\",\n","            output_dir=Config.LOG_DIR,\n","            measure_power_secs=10,\n","            log_level=\"warning\"\n","        )\n","\n","        print(\"\\nüèãÔ∏è Starting training...\")\n","        tracker.start()\n","        train_results = trainer.train()\n","        emissions = tracker.stop()\n","        emissions_data = tracker.final_emissions_data\n","        print(f\" Training complete!\")\n","        print(f\"   Training time: {train_results.metrics['train_runtime']/3600:.2f} hours\")\n","        print(f\"   CO2 emissions: {emissions_data.emissions:.6f} kg\")\n","\n","        # Save model\n","        print(f\"\\n Saving model to {self.output_dir}...\")\n","        trainer.save_model(self.output_dir)\n","        tokenizer.save_pretrained(self.output_dir)\n","\n","        # Evaluate\n","        evaluator = Evaluator(model, tokenizer, valid_subset)\n","        eval_results = evaluator.evaluate()\n","\n","        # Compile results\n","        result_entry = self.compile_results(\n","            train_results, eval_results, emissions_data,\n","            len(train_subset), len(valid_subset)\n","        )\n","\n","        # Save results\n","        self.save_results(result_entry)\n","\n","        # Cleanup\n","        del model, trainer\n","        torch.cuda.empty_cache()\n","\n","        return result_entry\n","\n","    def compile_results(self, train_results, eval_results, emissions_data,\n","                       num_train, num_valid):\n","        \"\"\"Compile all results into dictionary\"\"\"\n","        return {\n","            # Experiment info\n","            \"training_method\": \"Full Fine-Tuning\",\n","            \"model_name\": Config.MODEL_NAME,\n","            \"train_samples\": num_train,\n","            \"valid_samples\": num_valid,\n","            \"dataset_fraction\": self.fraction,\n","\n","            # Performance metrics\n","            \"f1_score\": eval_results[\"f1\"],\n","            \"exact_match\": eval_results[\"exact\"],\n","            \"eval_loss\": train_results.metrics.get(\"eval_loss\", None),\n","\n","            # Training time\n","            \"training_time_hours\": train_results.metrics[\"train_runtime\"] / 3600,\n","            \"duration_hours\": emissions_data.duration / 3600,\n","\n","            # Carbon emissions\n","            \"emissions_kg\": emissions_data.emissions,\n","            \"emissions_rate_kg_per_s\": emissions_data.emissions_rate,\n","\n","            # Energy consumption\n","            \"energy_consumed_kwh\": emissions_data.energy_consumed,\n","            \"cpu_energy_kwh\": emissions_data.cpu_energy,\n","            \"gpu_energy_kwh\": emissions_data.gpu_energy,\n","            \"ram_energy_kwh\": emissions_data.ram_energy,\n","\n","            # Power draw\n","            \"cpu_power_w\": emissions_data.cpu_power,\n","            \"gpu_power_w\": emissions_data.gpu_power,\n","            \"ram_power_w\": emissions_data.ram_power,\n","\n","            # System info\n","            \"cpu_model\": emissions_data.cpu_model,\n","            \"cpu_count\": emissions_data.cpu_count,\n","            \"gpu_model\": emissions_data.gpu_model,\n","            \"gpu_count\": emissions_data.gpu_count,\n","            \"ram_total_size_gb\": emissions_data.ram_total_size,\n","\n","            # Location\n","            \"country_name\": emissions_data.country_name,\n","            \"region\": emissions_data.region,\n","            \"pue\": emissions_data.pue,\n","        }\n","\n","    def save_results(self, result_entry):\n","        \"\"\"Save results to CSV\"\"\"\n","        df = pd.DataFrame([result_entry])\n","\n","        if os.path.exists(Config.RESULTS_CSV):\n","            df.to_csv(Config.RESULTS_CSV, mode=\"a\", header=False, index=False)\n","            print(f\"\\n Results appended to {Config.RESULTS_CSV}\")\n","        else:\n","            df.to_csv(Config.RESULTS_CSV, index=False)\n","            print(f\"\\n Results saved to {Config.RESULTS_CSV}\")\n","\n","        # Print summary\n","        self.print_summary(result_entry)\n","\n","    def print_summary(self, results):\n","        \"\"\"Print formatted summary\"\"\"\n","        print(f\"\\n{'='*80}\")\n","        print(f\" RESULTS SUMMARY FOR {self.fraction*100}% DATASET\")\n","        print(f\"{'='*80}\")\n","        print(f\"\\n Performance Metrics:\")\n","        print(f\"   F1 Score: {results['f1_score']:.4f}\")\n","        print(f\"   Exact Match: {results['exact_match']:.4f}\")\n","\n","        print(f\"\\n‚ö° Energy & Emissions:\")\n","        print(f\"   Total Energy: {results['energy_consumed_kwh']:.6f} kWh\")\n","        print(f\"   CO2 Emissions: {results['emissions_kg']:.6f} kg\")\n","        print(f\"   Training Time: {results['training_time_hours']:.2f} hours\")\n","\n","        print(f\"\\n Hardware:\")\n","        print(f\"   CPU: {results['cpu_model']} ({results['cpu_count']} cores)\")\n","        if results['gpu_model']:\n","            print(f\"   GPU: {results['gpu_model']} (x{results['gpu_count']})\")\n","        print(f\"{'='*80}\\n\")"]},{"cell_type":"markdown","source":["## MAIN EXECUTION"],"metadata":{"id":"LSbJQQ9W7i2V"}},{"cell_type":"code","execution_count":25,"metadata":{"id":"rb4cXGgh_VX_","executionInfo":{"status":"ok","timestamp":1763423742594,"user_tz":300,"elapsed":8,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"}}},"outputs":[],"source":["def main():\n","    \"\"\"Run training for all dataset fractions\"\"\"\n","    print(\"=\"*80)\n","    print(\"GPT-2 SQUAD V2 FINE-TUNING PIPELINE\")\n","    print(\"=\"*80)\n","\n","    all_results = []\n","\n","    for fraction in Config.DATASET_FRACTIONS:\n","            pipeline = TrainingPipeline(fraction)\n","            result = pipeline.train()\n","            all_results.append(result)\n","        #try:\n","\n","        #except Exception as e:\n","        #    print(f\"\\n Error training with {fraction*100}% data: {e}\")\n","        #    continue\n","\n","    print(\"\\n\" + \"=\"*25)\n","    print(\" ALL EXPERIMENTS COMPLETE!\")\n","    print(\"=\"*25)\n","    print(f\"\\n Results saved to: {Config.RESULTS_CSV}\")\n","\n","    # Display summary table\n","    if all_results:\n","        summary_df = pd.DataFrame(all_results)[\n","            [\"dataset_fraction\", \"f1_score\", \"exact_match\",\n","             \"training_time_hours\", \"emissions_kg\"]\n","        ]\n","        print(\"\\n Summary Table:\")\n","        print(summary_df.to_string(index=False))"]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Xga4bp-AqhbQ","executionInfo":{"status":"ok","timestamp":1763434636085,"user_tz":300,"elapsed":3510983,"user":{"displayName":"Shruti Elangovan","userId":"04158361977432504971"}},"outputId":"31390b54-274e-4a47-ab5c-835da7e36cb8"},"execution_count":26,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["================================================================================\n","GPT-2 SQUAD V2 FINE-TUNING PIPELINE\n","================================================================================\n","\n","================================================================================\n"," TRAINING WITH 25.0% OF SQUAD V2 DATA\n","================================================================================\n","\n"," Setting up gpt2...\n","  Preparing 25.0% of training data...\n","   Training samples: 32579\n","   Validation samples: 11873\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[codecarbon WARNING @ 23:57:00] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon INFO @ 23:57:00] [setup] RAM Tracking...\n","[codecarbon INFO @ 23:57:00] [setup] CPU Tracking...\n","[codecarbon WARNING @ 23:57:01] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n","[codecarbon WARNING @ 23:57:01] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n"," Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n","\n","[codecarbon INFO @ 23:57:01] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n","[codecarbon WARNING @ 23:57:01] No CPU tracking mode found. Falling back on CPU constant mode.\n","[codecarbon INFO @ 23:57:01] [setup] GPU Tracking...\n","[codecarbon INFO @ 23:57:01] Tracking Nvidia GPU via pynvml\n","[codecarbon INFO @ 23:57:01] The below tracking methods have been set up:\n","                RAM Tracking Method: RAM power estimation model\n","                CPU Tracking Method: global constant\n","                GPU Tracking Method: pynvml\n","            \n","[codecarbon INFO @ 23:57:01] >>> Tracker's metadata:\n","[codecarbon INFO @ 23:57:01]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n","[codecarbon INFO @ 23:57:01]   Python version: 3.12.12\n","[codecarbon INFO @ 23:57:01]   CodeCarbon version: 3.1.0\n","[codecarbon INFO @ 23:57:01]   Available RAM : 167.052 GB\n","[codecarbon INFO @ 23:57:01]   CPU count: 12 thread(s) in 1 physical CPU(s)\n","[codecarbon INFO @ 23:57:01]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n","[codecarbon INFO @ 23:57:01]   GPU count: 1\n","[codecarbon INFO @ 23:57:01]   GPU model: 1 x NVIDIA A100-SXM4-80GB\n","[codecarbon INFO @ 23:57:01] Emissions data (if any) will be saved to file /content/drive/MyDrive/Carbonemission/Version2/gpt2-squadv2-25/emissions.csv\n","[codecarbon WARNING @ 23:57:01] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon WARNING @ 23:57:02] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n","[codecarbon WARNING @ 23:57:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n"," Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n","\n","[codecarbon WARNING @ 23:57:02] No CPU tracking mode found. Falling back on CPU constant mode.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","üèãÔ∏è Starting training...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4074' max='4074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4074/4074 09:51, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.736000</td>\n","      <td>0.987594</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.458200</td>\n","      <td>0.958091</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" Training complete!\n","   Training time: 0.16 hours\n","   CO2 emissions: 0.023115 kg\n","\n"," Saving model to /content/drive/MyDrive/Carbonemission/Version2/gpt2-squadv2-25...\n","\n"," Evaluating model on validation set...\n","   Processing 0/11873...\n","   Processing 100/11873...\n","   Processing 200/11873...\n","   Processing 300/11873...\n","   Processing 400/11873...\n","   Processing 500/11873...\n","   Processing 600/11873...\n","   Processing 700/11873...\n","   Processing 800/11873...\n","   Processing 900/11873...\n","   Processing 1000/11873...\n","   Processing 1100/11873...\n","   Processing 1200/11873...\n","   Processing 1300/11873...\n","   Processing 1400/11873...\n","   Processing 1500/11873...\n","   Processing 1600/11873...\n","   Processing 1700/11873...\n","   Processing 1800/11873...\n","   Processing 1900/11873...\n","   Processing 2000/11873...\n","   Processing 2100/11873...\n","   Processing 2200/11873...\n","   Processing 2300/11873...\n","   Processing 2400/11873...\n","   Processing 2500/11873...\n","   Processing 2600/11873...\n","   Processing 2700/11873...\n","   Processing 2800/11873...\n","   Processing 2900/11873...\n","   Processing 3000/11873...\n","   Processing 3100/11873...\n","   Processing 3200/11873...\n","   Processing 3300/11873...\n","   Processing 3400/11873...\n","   Processing 3500/11873...\n","   Processing 3600/11873...\n","   Processing 3700/11873...\n","   Processing 3800/11873...\n","   Processing 3900/11873...\n","   Processing 4000/11873...\n","   Processing 4100/11873...\n","   Processing 4200/11873...\n","   Processing 4300/11873...\n","   Processing 4400/11873...\n","   Processing 4500/11873...\n","   Processing 4600/11873...\n","   Processing 4700/11873...\n","   Processing 4800/11873...\n","   Processing 4900/11873...\n","   Processing 5000/11873...\n","   Processing 5100/11873...\n","   Processing 5200/11873...\n","   Processing 5300/11873...\n","   Processing 5400/11873...\n","   Processing 5500/11873...\n","   Processing 5600/11873...\n","   Processing 5700/11873...\n","   Processing 5800/11873...\n","   Processing 5900/11873...\n","   Processing 6000/11873...\n","   Processing 6100/11873...\n","   Processing 6200/11873...\n","   Processing 6300/11873...\n","   Processing 6400/11873...\n","   Processing 6500/11873...\n","   Processing 6600/11873...\n","   Processing 6700/11873...\n","   Processing 6800/11873...\n","   Processing 6900/11873...\n","   Processing 7000/11873...\n","   Processing 7100/11873...\n","   Processing 7200/11873...\n","   Processing 7300/11873...\n","   Processing 7400/11873...\n","   Processing 7500/11873...\n","   Processing 7600/11873...\n","   Processing 7700/11873...\n","   Processing 7800/11873...\n","   Processing 7900/11873...\n","   Processing 8000/11873...\n","   Processing 8100/11873...\n","   Processing 8200/11873...\n","   Processing 8300/11873...\n","   Processing 8400/11873...\n","   Processing 8500/11873...\n","   Processing 8600/11873...\n","   Processing 8700/11873...\n","   Processing 8800/11873...\n","   Processing 8900/11873...\n","   Processing 9000/11873...\n","   Processing 9100/11873...\n","   Processing 9200/11873...\n","   Processing 9300/11873...\n","   Processing 9400/11873...\n","   Processing 9500/11873...\n","   Processing 9600/11873...\n","   Processing 9700/11873...\n","   Processing 9800/11873...\n","   Processing 9900/11873...\n","   Processing 10000/11873...\n","   Processing 10100/11873...\n","   Processing 10200/11873...\n","   Processing 10300/11873...\n","   Processing 10400/11873...\n","   Processing 10500/11873...\n","   Processing 10600/11873...\n","   Processing 10700/11873...\n","   Processing 10800/11873...\n","   Processing 10900/11873...\n","   Processing 11000/11873...\n","   Processing 11100/11873...\n","   Processing 11200/11873...\n","   Processing 11300/11873...\n","   Processing 11400/11873...\n","   Processing 11500/11873...\n","   Processing 11600/11873...\n","   Processing 11700/11873...\n","   Processing 11800/11873...\n","\n"," Evaluation complete!\n","   F1 Score: 17.5240\n","   Exact Match: 7.0580\n","\n"," Results saved to /content/drive/MyDrive/Carbonemission/Version2/gpt2_squadv2_results.csv\n","\n","================================================================================\n"," RESULTS SUMMARY FOR 25.0% DATASET\n","================================================================================\n","\n"," Performance Metrics:\n","   F1 Score: 17.5240\n","   Exact Match: 7.0580\n","\n","‚ö° Energy & Emissions:\n","   Total Energy: 0.049098 kWh\n","   CO2 Emissions: 0.023115 kg\n","   Training Time: 0.16 hours\n","\n"," Hardware:\n","   CPU: Intel(R) Xeon(R) CPU @ 2.20GHz (12 cores)\n","   GPU: 1 x NVIDIA A100-SXM4-80GB (x1)\n","================================================================================\n","\n","\n","================================================================================\n"," TRAINING WITH 50.0% OF SQUAD V2 DATA\n","================================================================================\n","\n"," Setting up gpt2...\n","  Preparing 50.0% of training data...\n","   Training samples: 65159\n","   Validation samples: 11873\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[codecarbon WARNING @ 00:45:21] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon INFO @ 00:45:21] [setup] RAM Tracking...\n","[codecarbon INFO @ 00:45:21] [setup] CPU Tracking...\n","[codecarbon WARNING @ 00:45:22] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n","[codecarbon WARNING @ 00:45:22] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n"," Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n","\n","[codecarbon INFO @ 00:45:22] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n","[codecarbon WARNING @ 00:45:22] No CPU tracking mode found. Falling back on CPU constant mode.\n","[codecarbon INFO @ 00:45:22] [setup] GPU Tracking...\n","[codecarbon INFO @ 00:45:22] Tracking Nvidia GPU via pynvml\n","[codecarbon INFO @ 00:45:22] The below tracking methods have been set up:\n","                RAM Tracking Method: RAM power estimation model\n","                CPU Tracking Method: global constant\n","                GPU Tracking Method: pynvml\n","            \n","[codecarbon INFO @ 00:45:22] >>> Tracker's metadata:\n","[codecarbon INFO @ 00:45:22]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n","[codecarbon INFO @ 00:45:22]   Python version: 3.12.12\n","[codecarbon INFO @ 00:45:22]   CodeCarbon version: 3.1.0\n","[codecarbon INFO @ 00:45:22]   Available RAM : 167.052 GB\n","[codecarbon INFO @ 00:45:22]   CPU count: 12 thread(s) in 1 physical CPU(s)\n","[codecarbon INFO @ 00:45:22]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n","[codecarbon INFO @ 00:45:22]   GPU count: 1\n","[codecarbon INFO @ 00:45:22]   GPU model: 1 x NVIDIA A100-SXM4-80GB\n","[codecarbon INFO @ 00:45:22] Emissions data (if any) will be saved to file /content/drive/MyDrive/Carbonemission/Version2/gpt2-squadv2-50/emissions.csv\n","[codecarbon WARNING @ 00:45:22] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon WARNING @ 00:45:24] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n","[codecarbon WARNING @ 00:45:24] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n"," Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n","\n","[codecarbon WARNING @ 00:45:24] No CPU tracking mode found. Falling back on CPU constant mode.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","üèãÔ∏è Starting training...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8146' max='8146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8146/8146 18:31, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.716200</td>\n","      <td>0.822139</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.470900</td>\n","      <td>0.785759</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" Training complete!\n","   Training time: 0.31 hours\n","   CO2 emissions: 0.044772 kg\n","\n"," Saving model to /content/drive/MyDrive/Carbonemission/Version2/gpt2-squadv2-50...\n","\n"," Evaluating model on validation set...\n","   Processing 0/11873...\n","   Processing 100/11873...\n","   Processing 200/11873...\n","   Processing 300/11873...\n","   Processing 400/11873...\n","   Processing 500/11873...\n","   Processing 600/11873...\n","   Processing 700/11873...\n","   Processing 800/11873...\n","   Processing 900/11873...\n","   Processing 1000/11873...\n","   Processing 1100/11873...\n","   Processing 1200/11873...\n","   Processing 1300/11873...\n","   Processing 1400/11873...\n","   Processing 1500/11873...\n","   Processing 1600/11873...\n","   Processing 1700/11873...\n","   Processing 1800/11873...\n","   Processing 1900/11873...\n","   Processing 2000/11873...\n","   Processing 2100/11873...\n","   Processing 2200/11873...\n","   Processing 2300/11873...\n","   Processing 2400/11873...\n","   Processing 2500/11873...\n","   Processing 2600/11873...\n","   Processing 2700/11873...\n","   Processing 2800/11873...\n","   Processing 2900/11873...\n","   Processing 3000/11873...\n","   Processing 3100/11873...\n","   Processing 3200/11873...\n","   Processing 3300/11873...\n","   Processing 3400/11873...\n","   Processing 3500/11873...\n","   Processing 3600/11873...\n","   Processing 3700/11873...\n","   Processing 3800/11873...\n","   Processing 3900/11873...\n","   Processing 4000/11873...\n","   Processing 4100/11873...\n","   Processing 4200/11873...\n","   Processing 4300/11873...\n","   Processing 4400/11873...\n","   Processing 4500/11873...\n","   Processing 4600/11873...\n","   Processing 4700/11873...\n","   Processing 4800/11873...\n","   Processing 4900/11873...\n","   Processing 5000/11873...\n","   Processing 5100/11873...\n","   Processing 5200/11873...\n","   Processing 5300/11873...\n","   Processing 5400/11873...\n","   Processing 5500/11873...\n","   Processing 5600/11873...\n","   Processing 5700/11873...\n","   Processing 5800/11873...\n","   Processing 5900/11873...\n","   Processing 6000/11873...\n","   Processing 6100/11873...\n","   Processing 6200/11873...\n","   Processing 6300/11873...\n","   Processing 6400/11873...\n","   Processing 6500/11873...\n","   Processing 6600/11873...\n","   Processing 6700/11873...\n","   Processing 6800/11873...\n","   Processing 6900/11873...\n","   Processing 7000/11873...\n","   Processing 7100/11873...\n","   Processing 7200/11873...\n","   Processing 7300/11873...\n","   Processing 7400/11873...\n","   Processing 7500/11873...\n","   Processing 7600/11873...\n","   Processing 7700/11873...\n","   Processing 7800/11873...\n","   Processing 7900/11873...\n","   Processing 8000/11873...\n","   Processing 8100/11873...\n","   Processing 8200/11873...\n","   Processing 8300/11873...\n","   Processing 8400/11873...\n","   Processing 8500/11873...\n","   Processing 8600/11873...\n","   Processing 8700/11873...\n","   Processing 8800/11873...\n","   Processing 8900/11873...\n","   Processing 9000/11873...\n","   Processing 9100/11873...\n","   Processing 9200/11873...\n","   Processing 9300/11873...\n","   Processing 9400/11873...\n","   Processing 9500/11873...\n","   Processing 9600/11873...\n","   Processing 9700/11873...\n","   Processing 9800/11873...\n","   Processing 9900/11873...\n","   Processing 10000/11873...\n","   Processing 10100/11873...\n","   Processing 10200/11873...\n","   Processing 10300/11873...\n","   Processing 10400/11873...\n","   Processing 10500/11873...\n","   Processing 10600/11873...\n","   Processing 10700/11873...\n","   Processing 10800/11873...\n","   Processing 10900/11873...\n","   Processing 11000/11873...\n","   Processing 11100/11873...\n","   Processing 11200/11873...\n","   Processing 11300/11873...\n","   Processing 11400/11873...\n","   Processing 11500/11873...\n","   Processing 11600/11873...\n","   Processing 11700/11873...\n","   Processing 11800/11873...\n","\n"," Evaluation complete!\n","   F1 Score: 16.0309\n","   Exact Match: 5.3988\n","\n"," Results appended to /content/drive/MyDrive/Carbonemission/Version2/gpt2_squadv2_results.csv\n","\n","================================================================================\n"," RESULTS SUMMARY FOR 50.0% DATASET\n","================================================================================\n","\n"," Performance Metrics:\n","   F1 Score: 16.0309\n","   Exact Match: 5.3988\n","\n","‚ö° Energy & Emissions:\n","   Total Energy: 0.095102 kWh\n","   CO2 Emissions: 0.044772 kg\n","   Training Time: 0.31 hours\n","\n"," Hardware:\n","   CPU: Intel(R) Xeon(R) CPU @ 2.20GHz (12 cores)\n","   GPU: 1 x NVIDIA A100-SXM4-80GB (x1)\n","================================================================================\n","\n","\n","================================================================================\n"," TRAINING WITH 80.0% OF SQUAD V2 DATA\n","================================================================================\n","\n"," Setting up gpt2...\n","  Preparing 80.0% of training data...\n","   Training samples: 104255\n","   Validation samples: 11873\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[codecarbon WARNING @ 01:47:34] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon INFO @ 01:47:34] [setup] RAM Tracking...\n","[codecarbon INFO @ 01:47:34] [setup] CPU Tracking...\n","[codecarbon WARNING @ 01:47:35] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n","[codecarbon WARNING @ 01:47:35] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n"," Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n","\n","[codecarbon INFO @ 01:47:35] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n","[codecarbon WARNING @ 01:47:35] No CPU tracking mode found. Falling back on CPU constant mode.\n","[codecarbon INFO @ 01:47:35] [setup] GPU Tracking...\n","[codecarbon INFO @ 01:47:35] Tracking Nvidia GPU via pynvml\n","[codecarbon INFO @ 01:47:35] The below tracking methods have been set up:\n","                RAM Tracking Method: RAM power estimation model\n","                CPU Tracking Method: global constant\n","                GPU Tracking Method: pynvml\n","            \n","[codecarbon INFO @ 01:47:35] >>> Tracker's metadata:\n","[codecarbon INFO @ 01:47:35]   Platform system: Linux-6.6.105+-x86_64-with-glibc2.35\n","[codecarbon INFO @ 01:47:35]   Python version: 3.12.12\n","[codecarbon INFO @ 01:47:35]   CodeCarbon version: 3.1.0\n","[codecarbon INFO @ 01:47:35]   Available RAM : 167.052 GB\n","[codecarbon INFO @ 01:47:35]   CPU count: 12 thread(s) in 1 physical CPU(s)\n","[codecarbon INFO @ 01:47:35]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n","[codecarbon INFO @ 01:47:35]   GPU count: 1\n","[codecarbon INFO @ 01:47:35]   GPU model: 1 x NVIDIA A100-SXM4-80GB\n","[codecarbon INFO @ 01:47:35] Emissions data (if any) will be saved to file /content/drive/MyDrive/Carbonemission/Version2/gpt2-squadv2-80/emissions.csv\n","[codecarbon WARNING @ 01:47:35] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon WARNING @ 01:47:37] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n","[codecarbon WARNING @ 01:47:37] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n"," Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n","\n","[codecarbon WARNING @ 01:47:37] No CPU tracking mode found. Falling back on CPU constant mode.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","üèãÔ∏è Starting training...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='5265' max='13032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 5265/13032 11:06 < 16:23, 7.90 it/s, Epoch 0.81/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13032' max='13032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13032/13032 28:58, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.671000</td>\n","      <td>0.794809</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.429300</td>\n","      <td>0.708025</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Training complete!\n","   Training time: 0.48 hours\n","   CO2 emissions: 0.070691 kg\n","\n"," Saving model to /content/drive/MyDrive/Carbonemission/Version2/gpt2-squadv2-80...\n","\n"," Evaluating model on validation set...\n","   Processing 0/11873...\n","   Processing 100/11873...\n","   Processing 200/11873...\n","   Processing 300/11873...\n","   Processing 400/11873...\n","   Processing 500/11873...\n","   Processing 600/11873...\n","   Processing 700/11873...\n","   Processing 800/11873...\n","   Processing 900/11873...\n","   Processing 1000/11873...\n","   Processing 1100/11873...\n","   Processing 1200/11873...\n","   Processing 1300/11873...\n","   Processing 1400/11873...\n","   Processing 1500/11873...\n","   Processing 1600/11873...\n","   Processing 1700/11873...\n","   Processing 1800/11873...\n","   Processing 1900/11873...\n","   Processing 2000/11873...\n","   Processing 2100/11873...\n","   Processing 2200/11873...\n","   Processing 2300/11873...\n","   Processing 2400/11873...\n","   Processing 2500/11873...\n","   Processing 2600/11873...\n","   Processing 2700/11873...\n","   Processing 2800/11873...\n","   Processing 2900/11873...\n","   Processing 3000/11873...\n","   Processing 3100/11873...\n","   Processing 3200/11873...\n","   Processing 3300/11873...\n","   Processing 3400/11873...\n","   Processing 3500/11873...\n","   Processing 3600/11873...\n","   Processing 3700/11873...\n","   Processing 3800/11873...\n","   Processing 3900/11873...\n","   Processing 4000/11873...\n","   Processing 4100/11873...\n","   Processing 4200/11873...\n","   Processing 4300/11873...\n","   Processing 4400/11873...\n","   Processing 4500/11873...\n","   Processing 4600/11873...\n","   Processing 4700/11873...\n","   Processing 4800/11873...\n","   Processing 4900/11873...\n","   Processing 5000/11873...\n","   Processing 5100/11873...\n","   Processing 5200/11873...\n","   Processing 5300/11873...\n","   Processing 5400/11873...\n","   Processing 5500/11873...\n","   Processing 5600/11873...\n","   Processing 5700/11873...\n","   Processing 5800/11873...\n","   Processing 5900/11873...\n","   Processing 6000/11873...\n","   Processing 6100/11873...\n","   Processing 6200/11873...\n","   Processing 6300/11873...\n","   Processing 6400/11873...\n","   Processing 6500/11873...\n","   Processing 6600/11873...\n","   Processing 6700/11873...\n","   Processing 6800/11873...\n","   Processing 6900/11873...\n","   Processing 7000/11873...\n","   Processing 7100/11873...\n","   Processing 7200/11873...\n","   Processing 7300/11873...\n","   Processing 7400/11873...\n","   Processing 7500/11873...\n","   Processing 7600/11873...\n","   Processing 7700/11873...\n","   Processing 7800/11873...\n","   Processing 7900/11873...\n","   Processing 8000/11873...\n","   Processing 8100/11873...\n","   Processing 8200/11873...\n","   Processing 8300/11873...\n","   Processing 8400/11873...\n","   Processing 8500/11873...\n","   Processing 8600/11873...\n","   Processing 8700/11873...\n","   Processing 8800/11873...\n","   Processing 8900/11873...\n","   Processing 9000/11873...\n","   Processing 9100/11873...\n","   Processing 9200/11873...\n","   Processing 9300/11873...\n","   Processing 9400/11873...\n","   Processing 9500/11873...\n","   Processing 9600/11873...\n","   Processing 9700/11873...\n","   Processing 9800/11873...\n","   Processing 9900/11873...\n","   Processing 10000/11873...\n","   Processing 10100/11873...\n","   Processing 10200/11873...\n","   Processing 10300/11873...\n","   Processing 10400/11873...\n","   Processing 10500/11873...\n","   Processing 10600/11873...\n","   Processing 10700/11873...\n","   Processing 10800/11873...\n","   Processing 10900/11873...\n","   Processing 11000/11873...\n","   Processing 11100/11873...\n","   Processing 11200/11873...\n","   Processing 11300/11873...\n","   Processing 11400/11873...\n","   Processing 11500/11873...\n","   Processing 11600/11873...\n","   Processing 11700/11873...\n","   Processing 11800/11873...\n","\n"," Evaluation complete!\n","   F1 Score: 15.7496\n","   Exact Match: 4.7755\n","\n"," Results appended to /content/drive/MyDrive/Carbonemission/Version2/gpt2_squadv2_results.csv\n","\n","================================================================================\n"," RESULTS SUMMARY FOR 80.0% DATASET\n","================================================================================\n","\n"," Performance Metrics:\n","   F1 Score: 15.7496\n","   Exact Match: 4.7755\n","\n","‚ö° Energy & Emissions:\n","   Total Energy: 0.150157 kWh\n","   CO2 Emissions: 0.070691 kg\n","   Training Time: 0.48 hours\n","\n"," Hardware:\n","   CPU: Intel(R) Xeon(R) CPU @ 2.20GHz (12 cores)\n","   GPU: 1 x NVIDIA A100-SXM4-80GB (x1)\n","================================================================================\n","\n","\n","=========================\n"," ALL EXPERIMENTS COMPLETE!\n","=========================\n","\n"," Results saved to: /content/drive/MyDrive/Carbonemission/Version2/gpt2_squadv2_results.csv\n","\n"," Summary Table:\n"," dataset_fraction  f1_score  exact_match  training_time_hours  emissions_kg\n","             0.25 17.523963     7.058031             0.164320      0.023115\n","             0.50 16.030853     5.398804             0.308925      0.044772\n","             0.80 15.749599     4.775541             0.483026      0.070691\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPUvSwRBynzW5aInnkWIS++"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}